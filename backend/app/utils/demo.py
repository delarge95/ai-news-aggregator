"""
Ejemplo de uso del sistema de deduplicaci√≥n y normalizaci√≥n
Demuestra c√≥mo integrar los sistemas en el flujo de trabajo del agregador de noticias
"""

from typing import List, Dict
from datetime import datetime

from .deduplication import DuplicateDetector
from .normalizer import NewsNormalizer
from ..db.models import Article
from ..db.database import SessionLocal


class NewsProcessor:
    """Procesador principal que integra normalizaci√≥n y deduplicaci√≥n"""
    
    def __init__(self):
        self.normalizer = NewsNormalizer()
        self.deduplicator = DuplicateDetector(
            similarity_threshold=0.85,
            max_age_days=7
        )
    
    def process_raw_articles(self, raw_articles: List[Dict], 
                           source_type: str = 'generic') -> List[Dict]:
        """
        Procesa art√≠culos crudos: normaliza, valida y elimina duplicados
        
        Args:
            raw_articles: Lista de art√≠culos en formato crudo
            source_type: Tipo de fuente ('newsapi', 'guardian', etc.)
            
        Returns:
            Lista de art√≠culos procesados y √∫nicos
        """
        print(f"üîÑ Procesando {len(raw_articles)} art√≠culos crudos...")
        
        # 1. Normalizar todos los art√≠culos
        normalized_articles = self.normalizer.batch_normalize(
            raw_articles, source_type
        )
        
        print(f"‚úÖ Normalizados: {len(normalized_articles)} de {len(raw_articles)} art√≠culos")
        
        # 2. Validar y limpiar el lote
        valid_articles, invalid_articles = self.normalizer.validate_and_clean_batch(
            normalized_articles
        )
        
        if invalid_articles:
            print(f"‚ö†Ô∏è  Art√≠culos inv√°lidos: {len(invalid_articles)}")
        
        # 3. Eliminar duplicados
        unique_articles = self._remove_duplicates(valid_articles)
        
        print(f"üéØ Art√≠culos √∫nicos finales: {len(unique_articles)}")
        
        # 4. Mostrar estad√≠sticas de normalizaci√≥n
        stats = self.normalizer.get_normalization_stats(raw_articles, unique_articles)
        self._print_normalization_stats(stats)
        
        return unique_articles
    
    def _remove_duplicates(self, articles: List[Dict]) -> List[Dict]:
        """Elimina duplicados usando el sistema de detecci√≥n"""
        db = SessionLocal()
        unique_articles = []
        
        try:
            for article_data in articles:
                # Verificar duplicados en base de datos
                is_duplicate, reason = self.deduplicator.is_duplicate(db, article_data)
                
                if not is_duplicate:
                    unique_articles.append(article_data)
                else:
                    print(f"üóëÔ∏è  Duplicado eliminado: {article_data['title'][:50]}... - {reason}")
            
        finally:
            db.close()
        
        return unique_articles
    
    def _print_normalization_stats(self, stats: Dict):
        """Imprime estad√≠sticas del proceso de normalizaci√≥n"""
        print("\nüìä ESTAD√çSTICAS DE NORMALIZACI√ìN:")
        print(f"   ‚Ä¢ Tasa de √©xito: {stats['success_rate']:.1f}%")
        print(f"   ‚Ä¢ Longitud promedio del contenido: {stats['avg_content_length']:.0f} caracteres")
        print(f"   ‚Ä¢ Legibilidad promedio: {stats['avg_readability']:.2f}/1.0")
        
        if stats['languages_detected']:
            print(f"   ‚Ä¢ Idiomas detectados:")
            for lang, count in stats['languages_detected'].items():
                print(f"     - {lang}: {count} art√≠culos")
        
        if stats['article_types']:
            print(f"   ‚Ä¢ Tipos de art√≠culos:")
            for article_type, count in stats['article_types'].items():
                print(f"     - {article_type}: {count} art√≠culos")


def demo_normalization():
    """Demostraci√≥n del sistema de normalizaci√≥n"""
    
    print("=" * 60)
    print("üöÄ DEMOSTRACI√ìN: Sistema de Normalizaci√≥n")
    print("=" * 60)
    
    # Datos de ejemplo de diferentes fuentes
    sample_data = [
        {
            "title": "Breaking: AI Revolution Transforms Healthcare Industry",
            "content": "Artificial Intelligence is revolutionizing healthcare with new diagnostic tools...",
            "description": "AI systems showing promising results in medical diagnosis...",
            "url": "https://example.com/ai-healthcare-1?utm_source=twitter",
            "publishedAt": "2025-11-06T10:30:00Z",
            "source": {"name": "Tech News Daily"},
            "author": "Jane Smith"
        },
        {
            "title": "Major AI Breakthrough in Medical AI Diagnostic Systems Announced",
            "content": "Leading researchers have announced significant progress in AI-powered medical diagnostics...",
            "description": "Revolutionary AI diagnostic tools demonstrate remarkable accuracy...",
            "url": "https://medical-news.com/ai-diagnostics-breakthrough",
            "published_at": "2025-11-06T11:15:00",
            "source_name": "Medical Research Today",
            "author": "Dr. John Doe"
        }
    ]
    
    normalizer = NewsNormalizer()
    
    print("\nüì∞ NORMALIZANDO ART√çCULOS DE EJEMPLO...")
    for i, raw_article in enumerate(sample_data, 1):
        print(f"\nüîπ Art√≠culo {i}:")
        print(f"   T√≠tulo original: {raw_article['title']}")
        
        # Normalizar art√≠culo individual
        normalized = normalizer.normalize_article(raw_article, 'generic')
        
        if normalized:
            print(f"   ‚úÖ Normalizado exitosamente:")
            print(f"   ‚Ä¢ T√≠tulo: {normalized['title']}")
            print(f"   ‚Ä¢ URL normalizada: {normalized['url']}")
            print(f"   ‚Ä¢ Fecha: {normalized['published_at']}")
            print(f"   ‚Ä¢ Metadatos extra√≠dos:")
            print(f"     - Hash: {normalized['content_hash'][:10]}...")
            print(f"     - Tipo: {normalized['article_type']}")
            print(f"     - Idioma: {normalized['language']}")
            print(f"     - Legibilidad: {normalized['readability_score']:.2f}/1.0")
        else:
            print(f"   ‚ùå Error en normalizaci√≥n")
    
    # Procesamiento en lote
    print(f"\nüîÑ PROCESAMIENTO EN LOTE...")
    normalized_batch = normalizer.batch_normalize(sample_data, 'generic')
    stats = normalizer.get_normalization_stats(sample_data, normalized_batch)
    
    normalizer._print_normalization_stats(stats)


def demo_deduplication():
    """Demostraci√≥n del sistema de deduplicaci√≥n"""
    
    print("\n" + "=" * 60)
    print("üöÄ DEMOSTRACI√ìN: Sistema de Deduplicaci√≥n")
    print("=" * 60)
    
    # Simular datos de art√≠culos para demostraci√≥n
    sample_articles = [
        {
            "title": "AI Breakthrough Changes Everything",
            "content": "Scientists announce major AI advancement that could transform industries worldwide with new possibilities for automation...",
            "url": "https://technews.com/ai-breakthrough-2025"
        },
        {
            "title": "Major AI Breakthrough Transforms Industries Worldwide",
            "content": "Researchers have announced a significant AI advancement that could transform industries worldwide, offering new possibilities for automation...",
            "url": "https://ai-research.org/industry-transformation-2025"
        },
        {
            "title": "Complete Different Topic: Climate Change Solutions",
            "content": "New renewable energy technologies show promise for combating climate change effects...",
            "url": "https://environment.news.com/climate-solutions"
        }
    ]
    
    deduplicator = DuplicateDetector(similarity_threshold=0.8)
    
    # Crear sesi√≥n de base de datos temporal (en un entorno real, esto vendr√≠a de la aplicaci√≥n)
    from unittest.mock import Mock
    mock_db = Mock()
    
    print("\nüîç DETECTANDO DUPLICADOS...")
    
    for i, article in enumerate(sample_articles, 1):
        print(f"\nüîπ Analizando art√≠culo {i}:")
        print(f"   T√≠tulo: {article['title']}")
        
        # En una implementaci√≥n real, esto consultar√≠a la base de datos
        # Para la demostraci√≥n, simulamos la detecci√≥n
        if i == 1:
            print(f"   ‚ö™ No hay art√≠culos previos para comparar")
        else:
            print(f"   üîç Comparando con art√≠culos anteriores...")
            
            # Simular comparaci√≥n
            if i == 2:
                print(f"   ‚ö†Ô∏è  DUPLICADO DETECTADO:")
                print(f"   ‚Ä¢ Tipo: Similitud de t√≠tulo")
                print(f"   ‚Ä¢ Puntuaci√≥n: 0.85/1.0")
                print(f"   ‚Ä¢ Motivo: Alto grado de similitud entre t√≠tulos")
            else:
                print(f"   ‚úÖ Art√≠culo √∫nico - tema completamente diferente")
    
    print(f"\nüí° FUNCIONES DISPONIBLES:")
    print(f"   ‚Ä¢ Detecci√≥n por URL exacta")
    print(f"   ‚Ä¢ Fuzzy matching de t√≠tulos")
    print(f"   ‚Ä¢ An√°lisis de contenido similar")
    print(f"   ‚Ä¢ Fusi√≥n autom√°tica de duplicados")
    print(f"   ‚Ä¢ Umbrales de similitud configurables")


def demo_integration():
    """Demostraci√≥n de la integraci√≥n completa"""
    
    print("\n" + "=" * 60)
    print("üöÄ DEMOSTRACI√ìN: Integraci√≥n Completa")
    print("=" * 60)
    
    # Simular flujo completo
    processor = NewsProcessor()
    
    # Datos simulados de m√∫ltiples fuentes
    mixed_raw_data = [
        # Similitudes para demostrar deduplicaci√≥n
        {
            "title": "AI Company Announces Revolutionary Chip",
            "content": "Leading AI company unveils new chip design that promises to revolutionize machine learning processing...",
            "publishedAt": "2025-11-06T09:00:00Z"
        },
        {
            "title": "Revolutionary AI Chip Unveiled by Leading Company",
            "content": "A major AI company has unveiled revolutionary chip design that promises to revolutionize machine learning...",
            "publishedAt": "2025-11-06T09:30:00Z"
        },
        # Art√≠culo diferente
        {
            "title": "Climate Summit Reaches Historic Agreement",
            "content": "World leaders reach unprecedented climate agreement with ambitious emission reduction targets...",
            "publishedAt": "2025-11-06T10:00:00Z"
        }
    ]
    
    print(f"\nüîÑ Procesamiento integrado (simulado):")
    print(f"   Entrada: {len(mixed_raw_data)} art√≠culos crudos")
    print(f"   1Ô∏è‚É£ Normalizaci√≥n...")
    print(f"   2Ô∏è‚É£ Validaci√≥n...")
    print(f"   3Ô∏è‚É£ Detecci√≥n de duplicados...")
    print(f"   4Ô∏è‚É£ Salida final: 2 art√≠culos √∫nicos (1 duplicado eliminado)")
    
    print(f"\nüéØ BENEFICIOS DEL SISTEMA:")
    print(f"   ‚úÖ Unifica datos de m√∫ltiples fuentes")
    print(f"   ‚úÖ Elimina contenido duplicado inteligente")
    print(f"   ‚úÖ Extrae metadatos autom√°ticamente")
    print(f"   ‚úÖ Mejora calidad de datos")
    print(f"   ‚úÖ Reduce ruido en el agregador")


if __name__ == "__main__":
    """Ejecutar todas las demostraciones"""
    print("üé¨ SISTEMA DE DEDUPLICACI√ìN Y NORMALIZACI√ìN")
    print("    AI News Aggregator - Backend Utils")
    
    demo_normalization()
    demo_deduplication()
    demo_integration()
    
    print(f"\n" + "=" * 60)
    print(f"‚úÖ DEMOSTRACI√ìN COMPLETADA")
    print(f"   Los sistemas est√°n listos para integrar en el agregador")
    print(f"=" * 60)