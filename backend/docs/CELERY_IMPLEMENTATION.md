# Sistema de Procesamiento As√≠ncrono con Celery

## üìã Resumen

Este documento describe la implementaci√≥n completa del sistema de procesamiento as√≠ncrono con Celery para el proyecto AI News Aggregator. El sistema maneja tareas de an√°lisis de art√≠culos, clasificaci√≥n tem√°tica, generaci√≥n de res√∫menes y obtenci√≥n autom√°tica de noticias.

## üèóÔ∏è Arquitectura del Sistema

### Componentes Principales

1. **Celery Application** (`celery_app.py`)
   - Configuraci√≥n centralizada de Celery
   - Definici√≥n de colas y routing de tareas
   - Configuraci√≥n de retry y monitoreo
   - Manejo de se√±ales para logging

2. **Workers Especializados**
   - `ai_analysis`: An√°lisis de art√≠culos con OpenAI
   - `ai_classification`: Clasificaci√≥n tem√°tica
   - `ai_summaries`: Generaci√≥n de res√∫menes
   - `news_fetch`: Obtenci√≥n de noticias
   - `general`: Tareas generales y mantenimiento

3. **Sistema de Colas**
   - Queue separada por tipo de tarea
   - Rate limiting por cola
   - Priorizaci√≥n de tareas

4. **Monitoreo y Mantenimiento**
   - Tareas de limpieza autom√°tica
   - M√©tricas del sistema
   - Health checks
   - Reportes semanales

## üöÄ Tareas Implementadas

### 1. An√°lisis de Art√≠culos (`article_tasks.py`)

#### `analyze_article_async()`
- **Prop√≥sito**: Analizar un art√≠culo individual usando OpenAI
- **Tipos de an√°lisis**: 
  - `basic`: An√°lisis b√°sico con resumen y categorizaci√≥n
  - `comprehensive`: An√°lisis completo con entidades y credibilidad
  - `sentiment`: An√°lisis espec√≠fico de sentimiento
- **Rate limit**: 10/minuto
- **Queue**: `ai_analysis`
- **Retry**: 3 intentos con backoff exponencial
- **Fallback**: An√°lisis tradicional sin OpenAI

#### Caracter√≠sticas:
- Manejo robusto de errores
- Logging detallado
- Timeouts configurables
- Resultados estructurados en JSON

### 2. Procesamiento en Lote (`batch_tasks.py`)

#### `batch_analyze_articles()`
- **Prop√≥sito**: Analizar m√∫ltiples art√≠culos en lotes
- **Paralelizaci√≥n**: ThreadPoolExecutor con workers configurables
- **Batch size**: Configurable (default: 5)
- **Rate limit**: 5/minuto
- **Queue**: `ai_analysis`
- **Caracter√≠sticas**:
  - Procesamiento secuencial de lotes para evitar sobrecarga
  - Estad√≠sticas detalladas de rendimiento
  - Manejo de art√≠culos fallidos

#### `process_pending_analyses()`
- **Prop√≥sito**: Procesar an√°lisis pendientes de la base de datos
- **Scheduler**: Cada 10 minutos v√≠a Celery Beat
- **Rate limit**: 2/minuto
- **Queue**: `ai_analysis`

### 3. Clasificaci√≥n de Temas (`classification_tasks.py`)

#### `classify_topics_batch()`
- **Prop√≥sito**: Clasificar temas de m√∫ltiples art√≠culos
- **Sistemas de clasificaci√≥n**:
  - `basic`: 5 categor√≠as principales
  - `comprehensive`: 8 categor√≠as extendidas
  - `custom`: Sistema personalizable
- **Rate limit**: 8/minuto
- **Queue**: `ai_classification`
- **Caracter√≠sticas**:
  - Algoritmo basado en palabras clave
  - Scores de confianza normalizados
  - Distribuci√≥n tem√°tica global

#### `update_classification_model()`
- **Prop√≥sito**: Actualizar modelo de clasificaci√≥n
- **Rate limit**: 1/hora
- **Queue**: `ai_classification`

### 4. Generaci√≥n de Res√∫menes (`summary_tasks.py`)

#### `generate_summaries_batch()`
- **Prop√≥sito**: Generar res√∫menes de m√∫ltiples art√≠culos
- **Tipos de resumen**:
  - `brief`: Muy conciso (<100 caracteres)
  - `executive`: Balanceado (150-200 caracteres)
  - `comprehensive`: Detallado (250-300 caracteres)
- **Rate limit**: 6/minuto
- **Queue**: `ai_summaries`
- **Caracter√≠sticas**:
  - Filtrado de art√≠culos v√°lidos
  - Compresi√≥n inteligente del texto
  - M√©todos extractivos como fallback

#### `generate_article_digest()`
- **Prop√≥sito**: Crear digest consolidado de m√∫ltiples art√≠culos
- **Tipos**: `hourly`, `daily`, `weekly`
- **Rate limit**: 10/hora
- **Queue**: `ai_summaries`

### 5. Obtenci√≥n de Noticias (`news_tasks.py`)

#### `fetch_latest_news()`
- **Prop√≥sito**: Obtener noticias de m√∫ltiples fuentes
- **Fuentes soportadas**: NewsAPI, The Guardian, NYTimes
- **Rate limit**: 2/minuto
- **Queue**: `news_fetch`
- **Caracter√≠sticas**:
  - Obtenci√≥n paralela de fuentes
  - Eliminaci√≥n de duplicados
  - Filtros por fuente y categor√≠a
  - Procesamiento autom√°tico encadenado

#### `search_news_task()`
- **Prop√≥sito**: Buscar noticias por query espec√≠fico
- **Rate limit**: 5/minuto
- **Queue**: `news_fetch`

#### `schedule_continuous_fetch()`
- **Prop√≥sito**: Programar obtenci√≥n continua
- **Rate limit**: 1/hora
- **Queue**: `news_fetch`

### 6. Monitoreo y Mantenimiento (`monitoring.py`)

#### `clean_old_task_results()`
- **Prop√≥sito**: Limpiar resultados antiguos
- **Retention**: Configurable (default: 7 d√≠as)
- **Rate limit**: 1/hora
- **Queue**: `maintenance`

#### `get_system_metrics()`
- **Prop√≥sito**: Recopilar m√©tricas del sistema
- **M√©tricas incluidas**:
  - Estado de workers de Celery
  - Estad√≠sticas de Redis
  - Uso de CPU y memoria
  - Tareas activas y pendientes
- **Rate limit**: 5/hora
- **Queue**: `maintenance`

#### `check_task_health()`
- **Prop√≥sito**: Verificar salud del sistema
- **Rate limit**: 10/hora
- **Queue**: `maintenance`

#### `generate_weekly_report()`
- **Prop√≥sito**: Generar reporte semanal
- **Rate limit**: 1/semana
- **Queue**: `maintenance`

## ‚öôÔ∏è Configuraci√≥n

### Variables de Entorno

```bash
# Redis Configuration
REDIS_URL=redis://localhost:6379
CELERY_BROKER_URL=redis://localhost:6379
CELERY_RESULT_BACKEND=redis://localhost:6379

# OpenAI Configuration (opcional)
OPENAI_API_KEY=your_openai_api_key
OPENAI_MODEL=gpt-3.5-turbo

# API Keys para noticias
NEWSAPI_KEY=your_newsapi_key
GUARDIAN_API_KEY=your_guardian_api_key
NYTIMES_API_KEY=your_nytimes_api_key
```

### Configuraci√≥n de Cola en `celery_app.py`

```python
# Definici√≥n de colas
task_queues=(
    Queue('default', routing_key='default'),
    Queue('ai_analysis', routing_key='ai_analysis'),
    Queue('ai_classification', routing_key='ai_classification'),
    Queue('ai_summaries', routing_key='ai_summaries'),
    Queue('news_fetch', routing_key='news_fetch'),
    Queue('maintenance', routing_key='maintenance')
)

# Rate limiting por tarea
task_annotations={
    'app.tasks.article_tasks.analyze_article_async': {'rate_limit': '10/m'},
    'app.tasks.batch_tasks.batch_analyze_articles': {'rate_limit': '5/m'},
    'app.tasks.classification_tasks.classify_topics_batch': {'rate_limit': '8/m'},
    'app.tasks.summary_tasks.generate_summaries_batch': {'rate_limit': '6/m'},
}
```

## üìä Scripts de Gesti√≥n

### `start_celery.sh`
Inicia todos los workers y servicios:
- Workers especializados por tipo de tarea
- Celery Beat para tareas programadas
- Flower para monitoreo web (puerto 5555)
- Logging detallado por componente

### `stop_celery.sh`
Detiene todos los servicios de forma segura:
- Terminaci√≥n graceful de procesos
- Limpieza de archivos PID
- Forzado de procesos estancados

### `status_celery.sh`
Muestra estado completo del sistema:
- Estado de workers y procesos
- Uso de memoria y recursos
- Logs recientes
- M√©tricas de colas

## üîß Uso y Ejemplos

### Ejecutar An√°lisis Individual

```python
from app.tasks import analyze_article_async

# Analizar un art√≠culo
result = analyze_article_async.delay(article_data, 'comprehensive')
analysis_result = result.get(timeout=300)  # 5 minutos timeout
```

### Procesar Lote de Art√≠culos

```python
from app.tasks import batch_analyze_articles

# Analizar m√∫ltiples art√≠culos
result = batch_analyze_articles.delay(articles, 'comprehensive', batch_size=5)
batch_result = result.get(timeout=1800)  # 30 minutos timeout
```

### Clasificar Temas

```python
from app.tasks import classify_topics_batch

# Clasificar art√≠culos por temas
result = classify_topics_batch.delay(articles, 'comprehensive')
classification_result = result.get(timeout=600)
```

### Generar Res√∫menes

```python
from app.tasks import generate_summaries_batch

# Generar res√∫menes ejecutivos
result = generate_summaries_batch.delay(articles, 'executive')
summary_result = result.get(timeout=900)
```

### Obtener Noticias

```python
from app.tasks import fetch_latest_news

# Obtener √∫ltimas noticias
result = fetch_latest_news.delay(limit_per_source=20)
news_result = result.get(timeout=300)
```

## üìà Monitoreo

### Flower (Monitoreo Web)
- **URL**: http://localhost:5555
- **Caracter√≠sticas**:
  - Dashboard en tiempo real
  - Lista de tareas activas/pendientes/completadas
  - M√©tricas de workers
  - Gr√°ficos de rendimiento

### Logs Estructurados
```
logs/
‚îú‚îÄ‚îÄ ai_analysis.log
‚îú‚îÄ‚îÄ ai_classification.log
‚îú‚îÄ‚îÄ ai_summaries.log
‚îú‚îÄ‚îÄ news_fetch.log
‚îú‚îÄ‚îÄ general.log
‚îú‚îÄ‚îÄ beat.log
‚îî‚îÄ‚îÄ flower.log
```

### M√©tricas de Sistema
- **CPU y Memoria**: Monitoreo continuo con psutil
- **Redis**: M√©tricas de conexi√≥n y memoria
- **Celery**: Tareas activas, pendientes y completadas
- **Rate Limiting**: Estad√≠sticas de uso por cola

## üõ°Ô∏è Manejo de Errores

### Estrategias de Retry
- **Exponencial backoff**: 1min ‚Üí 2min ‚Üí 4min ‚Üí 7min
- **M√°ximo 3 intentos** por tarea
- **Jitter deshabilitado** para consistencia
- **Rate limiting** por tipo de tarea

### Fallbacks
- **An√°lisis sin OpenAI**: M√©todos extractivos tradicionales
- **Redis desconectado**: Modo degradado con logging
- **API keys faltantes**: Funcionalidad b√°sica sin AI

### Logging y Alertas
- **Niveles**: INFO, WARNING, ERROR, CRITICAL
- **Correlaci√≥n**: Task ID en todos los logs
- **Contexto**: Informaci√≥n detallada de errores
- **Rotaci√≥n**: Logs rotativos autom√°ticos

## üîÑ Tareas Programadas (Celery Beat)

```python
celery_app.conf.beat_schedule = {
    'fetch-latest-news': {
        'task': 'app.tasks.news_tasks.fetch_latest_news',
        'schedule': 300.0,  # cada 5 minutos
        'options': {'queue': 'news_fetch'}
    },
    'analyze-pending-articles': {
        'task': 'app.tasks.batch_tasks.process_pending_analyses',
        'schedule': 600.0,  # cada 10 minutos
        'options': {'queue': 'ai_analysis'}
    },
    'clean-old-results': {
        'task': 'app.tasks.monitoring.clean_old_task_results',
        'schedule': 3600.0,  # cada hora
        'options': {'queue': 'maintenance'}
    }
}
```

## üìã Dependencias

### Core Requirements
```
celery[redis]==5.3.4
kombu==5.3.4
billiard==4.2.0
redis==5.0.1
openai==1.3.7
```

### Monitoreo
```
flower==1.0.0
psutil==5.9.0
```

### Procesamiento de Texto
```
nltk==3.8.1
textblob==0.17.1
```

## üöÄ Mejores Pr√°cticas

### 1. **Gesti√≥n de Memoria**
- Workers con `max_tasks_per_child=1000`
- Procesamiento en lotes peque√±os
- Limpieza peri√≥dica de resultados

### 2. **Rate Limiting**
- L√≠mites conservadores por API
- Backoff exponencial en fallas
- Monitoreo de uso de cuotas

### 3. **Escalabilidad**
- Workers especializados por cola
- Concurrencia configurable
- Horizontal scaling por tipo de tarea

### 4. **Observabilidad**
- Logging estructurado con correlaciones
- M√©tricas de sistema en tiempo real
- Health checks autom√°ticos
- Reportes de rendimiento

### 5. **Mantenimiento**
- Limpieza autom√°tica de datos antiguos
- Verificaci√≥n de salud peri√≥dica
- Actualizaciones de modelos de clasificaci√≥n
- Optimizaci√≥n de consultas

## üîß Troubleshooting

### Problemas Comunes

1. **Workers no se inician**
   - Verificar Redis est√© ejecut√°ndose
   - Revisar variables de entorno
   - Validar configuraci√≥n de Celery

2. **Tareas fallan constantemente**
   - Verificar API keys
   - Revisar l√≠mites de rate
   - Analizar logs de error

3. **Alta latencia**
   - Reducir concurrencia
   - Optimizar par√°metros de retry
   - Escalar horizontalmente

4. **Redis sin memoria**
   - Aumentar configuraci√≥n de Redis
   - Reducir retenci√≥n de resultados
   - Implementar limpieza m√°s frecuente

### Comandos de Diagn√≥stico

```bash
# Verificar estado
./status_celery.sh

# Monitorear logs en tiempo real
tail -f logs/ai_analysis.log

# Verificar tareas en Flower
curl http://localhost:5555/api/tasks

# Verificar colas de Redis
redis-cli LLEN celery
```

## üìà M√©tricas de Rendimiento

### Benchmarks T√≠picos
- **An√°lisis individual**: 2-5 segundos
- **Procesamiento en lote**: 10-15 segundos por art√≠culo
- **Clasificaci√≥n**: 0.5-2 segundos por art√≠culo
- **Generaci√≥n de res√∫menes**: 1-3 segundos por art√≠culo

### Throughput Esperado
- **An√°lisis**: 100-200 art√≠culos/hora por worker
- **Clasificaci√≥n**: 300-500 art√≠culos/hora por worker
- **Res√∫menes**: 200-400 art√≠culos/hora por worker

## üîÆ Extensiones Futuras

1. **Machine Learning Models**
   - Modelos propios de clasificaci√≥n
   - An√°lisis de sentimiento avanzado
   - Detecci√≥n de fake news

2. **Integraciones Adicionales**
   - M√°s fuentes de noticias
   - Notificaciones push
   - APIs de terceros

3. **Optimizaci√≥n**
   - Cache de resultados
   - Compresi√≥n de datos
   - Paralelizaci√≥n avanzada

4. **Analytics**
   - Dashboards interactivos
   - M√©tricas de usuario
   - An√°lisis de tendencias

---

*Documentaci√≥n generada para AI News Aggregator v1.0*
*√öltima actualizaci√≥n: 2025-11-06*