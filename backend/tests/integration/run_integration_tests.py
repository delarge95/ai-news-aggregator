#!/bin/bash
"""
Script de ejecuciÃ³n de tests de integraciÃ³n
Run integration tests script

Este script facilita la ejecuciÃ³n de todos los tests de integraciÃ³n
con diferentes configuraciones y opciones.
"""

import os
import sys
import subprocess
import argparse
from pathlib import Path

# ConfiguraciÃ³n
PROJECT_ROOT = Path(__file__).parent.parent
INTEGRATION_TESTS_DIR = PROJECT_ROOT / "tests" / "integration"

def run_command(cmd, description, check=True):
    """Ejecutar comando con logging"""
    print(f"\n{'='*60}")
    print(f"ğŸ”§ {description}")
    print(f"Comando: {cmd}")
    print(f"{'='*60}")
    
    try:
        result = subprocess.run(cmd, shell=True, cwd=PROJECT_ROOT, check=check, capture_output=True, text=True)
        if result.stdout:
            print("ğŸ“¤ STDOUT:")
            print(result.stdout)
        if result.stderr:
            print("ğŸ“¥ STDERR:")
            print(result.stderr)
        return result.returncode == 0
    except subprocess.CalledProcessError as e:
        print(f"âŒ Error ejecutando comando: {e}")
        if e.stdout:
            print("ğŸ“¤ STDOUT:")
            print(e.stdout)
        if e.stderr:
            print("ğŸ“¥ STDERR:")
            print(e.stderr)
        return False

def check_environment():
    """Verificar entorno de testing"""
    print("ğŸ” Verificando entorno de testing...")
    
    # Verificar directorio de tests
    if not INTEGRATION_TESTS_DIR.exists():
        print(f"âŒ Directorio de tests no encontrado: {INTEGRATION_TESTS_DIR}")
        return False
    
    # Verificar archivos de tests
    test_files = [
        "test_api_integration.py",
        "test_database_integration.py", 
        "test_external_api_integration.py",
        "test_ai_integration.py",
        "test_cache_integration.py"
    ]
    
    for test_file in test_files:
        test_path = INTEGRATION_TESTS_DIR / test_file
        if test_path.exists():
            print(f"âœ… {test_file}")
        else:
            print(f"âŒ {test_file} no encontrado")
            return False
    
    return True

def setup_test_environment():
    """Configurar entorno de testing"""
    print("âš™ï¸ Configurando entorno de testing...")
    
    env_vars = {
        "TESTING": "true",
        "DATABASE_URL": "sqlite+aiosqlite:///:memory:",
        "REDIS_URL": "redis://localhost:6379/15",
        "PYTHONPATH": str(PROJECT_ROOT)
    }
    
    # Exportar variables de entorno
    for key, value in env_vars.items():
        os.environ[key] = value
        print(f"âœ… {key}={value}")
    
    return True

def install_dependencies():
    """Instalar dependencias de testing"""
    print("ğŸ“¦ Instalando dependencias de testing...")
    
    deps = [
        "pytest>=7.4.0",
        "pytest-asyncio>=0.21.0",
        "pytest-cov>=4.1.0",
        "pytest-xdist>=3.3.0",
        "httpx>=0.24.0",
        "redis>=4.6.0",
        "asyncpg>=0.28.0",
        "aiosqlite>=0.19.0"
    ]
    
    for dep in deps:
        cmd = f"pip install {dep}"
        if not run_command(cmd, f"Instalando {dep}", check=False):
            print(f"âš ï¸ Advertencia: No se pudo instalar {dep}")
    
    return True

def run_basic_tests():
    """Ejecutar tests bÃ¡sicos (sin dependencias externas)"""
    print("ğŸ§ª Ejecutando tests bÃ¡sicos...")
    
    cmd = f"""
    pytest tests/integration/ -v \
        --tb=short \
        --disable-warnings \
        -m "not slow and not requires_api_key" \
        --maxfail=3 \
        --durations=10
    """
    
    return run_command(cmd, "Tests de integraciÃ³n bÃ¡sicos")

def run_api_tests():
    """Ejecutar tests de API"""
    print("ğŸŒ Ejecutando tests de API...")
    
    cmd = f"""
    pytest tests/integration/test_api_integration.py -v \
        --tb=short \
        --disable-warnings \
        -m "integration and api"
    """
    
    return run_command(cmd, "Tests de API")

def run_database_tests():
    """Ejecutar tests de base de datos"""
    print("ğŸ—„ï¸ Ejecutando tests de base de datos...")
    
    cmd = f"""
    pytest tests/integration/test_database_integration.py -v \
        --tb=short \
        --disable-warnings \
        -m "integration and database"
    """
    
    return run_command(cmd, "Tests de base de datos")

def run_cache_tests():
    """Ejecutar tests de cache"""
    print("âš¡ Ejecutando tests de cache...")
    
    cmd = f"""
    pytest tests/integration/test_cache_integration.py -v \
        --tb=short \
        --disable-warnings \
        -m "integration and redis"
    """
    
    return run_command(cmd, "Tests de cache")

def run_ai_tests():
    """Ejecutar tests de IA"""
    print("ğŸ¤– Ejecutando tests de IA...")
    
    cmd = f"""
    pytest tests/integration/test_ai_integration.py -v \
        --tb=short \
        --disable-warnings \
        -m "integration and ai"
    """
    
    return run_command(cmd, "Tests de IA")

def run_external_api_tests(mock_only=True):
    """Ejecutar tests de APIs externas"""
    print(f"ğŸ”Œ Ejecutando tests de APIs externas ({'mockeadas' if mock_only else 'reales'})...")
    
    if mock_only:
        cmd = f"""
        pytest tests/integration/test_external_api_integration.py -v \
            --tb=short \
            --disable-warnings \
            -m "integration and external_api and not requires_api_key"
        """
    else:
        cmd = f"""
        ENABLE_API_TESTS=1 pytest tests/integration/test_external_api_integration.py -v \
            --tb=short \
            --disable-warnings \
            -m "integration and external_api"
        """
    
    return run_command(cmd, f"Tests de APIs externas ({'mockeadas' if mock_only else 'reales'})")

def run_performance_tests():
    """Ejecutar tests de performance"""
    print("âš¡ Ejecutando tests de performance...")
    
    cmd = f"""
    pytest tests/integration/ -v \
        --tb=short \
        -m "integration and performance" \
        --benchmark-only \
        --benchmark-sort=mean
    """
    
    return run_command(cmd, "Tests de performance")

def run_all_tests():
    """Ejecutar todos los tests de integraciÃ³n"""
    print("ğŸš€ Ejecutando todos los tests de integraciÃ³n...")
    
    cmd = f"""
    pytest tests/integration/ -v \
        --tb=short \
        --cov=app \
        --cov-report=html \
        --cov-report=term \
        --cov-fail-under=70 \
        --maxfail=5 \
        --durations=20
    """
    
    return run_command(cmd, "Todos los tests de integraciÃ³n")

def run_tests_with_coverage():
    """Ejecutar tests con cobertura"""
    print("ğŸ“Š Ejecutando tests con anÃ¡lisis de cobertura...")
    
    cmd = f"""
    pytest tests/integration/ \
        --cov=app \
        --cov-report=html:htmlcov \
        --cov-report=xml:coverage.xml \
        --cov-report=term-missing \
        --cov-fail-under=75 \
        --cov-branch
    """
    
    success = run_command(cmd, "Tests con cobertura")
    
    if success:
        print("\nğŸ“ˆ Reporte de cobertura generado:")
        print(f"  - HTML: {PROJECT_ROOT}/htmlcov/index.html")
        print(f"  - XML: {PROJECT_ROOT}/coverage.xml")
        print(f"  - Terminal: mostrado arriba")
    
    return success

def run_parallel_tests():
    """Ejecutar tests en paralelo"""
    print("âš¡ Ejecutando tests en paralelo...")
    
    cmd = f"""
    pytest tests/integration/ -n auto \
        -v \
        --tb=short \
        --maxfail=3 \
        --durations=10
    """
    
    return run_command(cmd, "Tests en paralelo")

def main():
    """FunciÃ³n principal"""
    parser = argparse.ArgumentParser(description="Script de tests de integraciÃ³n")
    parser.add_argument("--target", choices=[
        "basic", "api", "database", "cache", "ai", 
        "external-mock", "external-real", "performance",
        "all", "coverage", "parallel"
    ], default="basic", help="Tipo de tests a ejecutar")
    parser.add_argument("--install-deps", action="store_true", help="Instalar dependencias")
    parser.add_argument("--verbose", "-v", action="store_true", help="Output verbose")
    parser.add_argument("--fail-fast", action="store_true", help="Parar en el primer fallo")
    
    args = parser.parse_args()
    
    print("ğŸ§ª AI News Aggregator - Tests de IntegraciÃ³n")
    print(f"ğŸ“ Directorio: {INTEGRATION_TESTS_DIR}")
    print(f"ğŸ¯ Target: {args.target}")
    
    # Verificar entorno
    if not check_environment():
        print("âŒ Error verificando entorno")
        sys.exit(1)
    
    # Instalar dependencias si se solicita
    if args.install_deps:
        if not install_dependencies():
            print("âŒ Error instalando dependencias")
            sys.exit(1)
    
    # Configurar entorno
    if not setup_test_environment():
        print("âŒ Error configurando entorno")
        sys.exit(1)
    
    # Definir comandos segÃºn el target
    test_commands = {
        "basic": run_basic_tests,
        "api": run_api_tests,
        "database": run_database_tests,
        "cache": run_cache_tests,
        "ai": run_ai_tests,
        "external-mock": lambda: run_external_api_tests(mock_only=True),
        "external-real": lambda: run_external_api_tests(mock_only=False),
        "performance": run_performance_tests,
        "all": run_all_tests,
        "coverage": run_tests_with_coverage,
        "parallel": run_parallel_tests
    }
    
    # Ejecutar tests
    if args.target in test_commands:
        success = test_commands[args.target]()
        
        if success:
            print(f"\nâœ… Tests de {args.target} completados exitosamente")
            sys.exit(0)
        else:
            print(f"\nâŒ Tests de {args.target} fallaron")
            sys.exit(1)
    else:
        print(f"âŒ Target desconocido: {args.target}")
        sys.exit(1)

if __name__ == "__main__":
    # Verificar que estamos en el directorio correcto
    if not (PROJECT_ROOT / "app").exists():
        print("âŒ Error: Ejecutar desde el directorio backend del proyecto")
        print(f"Directorio actual: {os.getcwd()}")
        print(f"Directorio esperado: {PROJECT_ROOT}")
        sys.exit(1)
    
    main()